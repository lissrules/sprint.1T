{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45100ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.3.1`\n",
    "import $ivy.`sh.almond::almond-spark:0.13.2`\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad290245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{approx_count_distinct, col, countDistinct, percent_rank}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{BooleanType, IntegerType, LongType, StringType, StructField, StructType}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.{approx_count_distinct, col, countDistinct, percent_rank}\n",
    "import org.apache.spark.sql.types.{BooleanType, IntegerType, LongType, StringType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf7f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:49:22 INFO SparkContext: Running Spark version 3.3.1\n",
      "22/12/01 18:49:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/01 18:49:22 INFO ResourceUtils: ==============================================================\n",
      "22/12/01 18:49:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/12/01 18:49:22 INFO ResourceUtils: ==============================================================\n",
      "22/12/01 18:49:22 INFO SparkContext: Submitted application: SparkScala\n",
      "22/12/01 18:49:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/12/01 18:49:22 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/12/01 18:49:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/12/01 18:49:22 INFO SecurityManager: Changing view acls to: root\n",
      "22/12/01 18:49:23 INFO SecurityManager: Changing modify acls to: root\n",
      "22/12/01 18:49:23 INFO SecurityManager: Changing view acls groups to: \n",
      "22/12/01 18:49:23 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/12/01 18:49:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "22/12/01 18:49:23 INFO Utils: Successfully started service 'sparkDriver' on port 37761.\n",
      "22/12/01 18:49:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/12/01 18:49:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/12/01 18:49:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/12/01 18:49:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/12/01 18:49:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/12/01 18:49:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-659848c5-047c-46da-8abe-aa3a2d414e5d\n",
      "22/12/01 18:49:24 INFO MemoryStore: MemoryStore started with capacity 656.4 MiB\n",
      "22/12/01 18:49:24 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/12/01 18:49:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/12/01 18:49:24 INFO Executor: Starting executor ID driver on host 13de6c50d362\n",
      "22/12/01 18:49:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "22/12/01 18:49:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36135.\n",
      "22/12/01 18:49:24 INFO NettyBlockTransferService: Server created on 13de6c50d362:36135\n",
      "22/12/01 18:49:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/12/01 18:49:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 13de6c50d362, 36135, None)\n",
      "22/12/01 18:49:24 INFO BlockManagerMasterEndpoint: Registering block manager 13de6c50d362:36135 with 656.4 MiB RAM, BlockManagerId(driver, 13de6c50d362, 36135, None)\n",
      "22/12/01 18:49:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 13de6c50d362, 36135, None)\n",
      "22/12/01 18:49:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 13de6c50d362, 36135, None)\n",
      "22/12/01 18:49:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/12/01 18:49:26 INFO SharedState: Warehouse path is 'file:/opt/workspace/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@42beb641\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mstruct\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"id\"\u001b[39m, IntegerType, false, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"timestamp\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"type\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"page_id\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"tag\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"sign\"\u001b[39m, BooleanType, true, {})\n",
       ")\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  [1,1667927426,visit,77,sport,true],\n",
       "  [1,1667327495,click,77,sport,true],\n",
       "  [1,1667627576,scroll,77,sport,true],\n",
       "  [1,1667427626,move,78,market,true],\n",
       "  [1,1667135426,visit,77,sport,true],\n",
       "  [1,1667225495,click,77,sport,true],\n",
       "  [1,1667685576,scroll,77,sport,true],\n",
       "  [1,1667696626,move,78,market,true],\n",
       "  [2,1667718426,visit,74,food,false],\n",
       "  [2,1668128495,click,74,food,false],\n",
       "  [2,1664928576,scroll,74,food,false],\n",
       "  [2,1667378626,move,77,sport,false],\n",
       "  [3,1667993426,visit,78,market,true],\n",
       "  [3,1667229495,click,78,market,true],\n",
       "  [3,1667329576,scroll,78,market,true],\n",
       "  [3,1667829626,move,77,sport,true],\n",
       "  [4,1667822433,visit,74,food,false],\n",
       "  [4,1667722455,click,74,food,false],\n",
       "  [4,1667522526,scroll,74,food,false],\n",
       "  [4,1667593021,click,78,market,false],\n",
       "  [4,1667593223,scroll,78,market,false],\n",
       "  [4,1667553727,click,79,tools,false],\n",
       "  [5,1667443336,visit,78,market,false],\n",
       "  [6,1667913248,visit,78,market,false],\n",
       "  [7,1667923459,visit,74,food,false],\n",
       "  [8,1667873542,visit,74,food,false],\n",
       "  [8,1667623942,visit,74,food,false],\n",
       "  [9,1667623314,visit,74,food,false],\n",
       "  [10,1667634657,visit,79,tools,false],\n",
       "  [11,1667643696,visit,77,sport,false],\n",
       "  [12,1667629904,visit,77,sport,false]\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, timestamp: int ... 4 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().master(\"local[1]\")\n",
    "  .appName(\"SparkScala\")\n",
    "  .getOrCreate()\n",
    "import spark.implicits._\n",
    "val struct = (new StructType)\n",
    "  .add(StructField(\"id\", IntegerType, false))\n",
    "  .add(StructField(\"timestamp\", IntegerType, true))\n",
    "  .add(StructField(\"type\", StringType, true))\n",
    "  .add(StructField(\"page_id\", IntegerType, true))\n",
    "  .add(StructField(\"tag\", StringType, true))\n",
    "  .add(StructField(\"sign\", BooleanType, true))\n",
    "val data = Seq(\n",
    "  Row(1, 1667927426, \"visit\", 77, \"sport\", true),\n",
    "  Row(1, 1667327495, \"click\", 77, \"sport\", true),\n",
    "  Row(1, 1667627576, \"scroll\", 77, \"sport\", true),\n",
    "  Row(1, 1667427626, \"move\", 78, \"market\", true),\n",
    "  Row(1, 1667135426, \"visit\", 77, \"sport\", true),\n",
    "  Row(1, 1667225495, \"click\", 77, \"sport\", true),\n",
    "  Row(1, 1667685576, \"scroll\", 77, \"sport\", true),\n",
    "  Row(1, 1667696626, \"move\", 78, \"market\", true),\n",
    "  Row(2, 1667718426, \"visit\", 74, \"food\", false),\n",
    "  Row(2, 1668128495, \"click\", 74, \"food\", false),\n",
    "  Row(2, 1664928576, \"scroll\", 74, \"food\", false),\n",
    "  Row(2, 1667378626, \"move\", 77, \"sport\", false),\n",
    "  Row(3, 1667993426, \"visit\", 78, \"market\", true),\n",
    "  Row(3, 1667229495, \"click\", 78, \"market\", true),\n",
    "  Row(3, 1667329576, \"scroll\", 78, \"market\", true),\n",
    "  Row(3, 1667829626, \"move\", 77, \"sport\", true),\n",
    "  Row(4, 1667822433, \"visit\", 74, \"food\", false),\n",
    "  Row(4, 1667722455, \"click\", 74, \"food\", false),\n",
    "  Row(4, 1667522526, \"scroll\", 74, \"food\", false),\n",
    "  Row(4, 1667593021, \"click\", 78, \"market\", false),\n",
    "  Row(4, 1667593223, \"scroll\", 78, \"market\", false),\n",
    "  Row(4, 1667553727, \"click\", 79, \"tools\", false),\n",
    "  Row(5, 1667443336, \"visit\", 78, \"market\", false),\n",
    "  Row(6, 1667913248, \"visit\", 78, \"market\", false),\n",
    "  Row(7, 1667923459, \"visit\", 74, \"food\", false),\n",
    "  Row(8, 1667873542, \"visit\", 74, \"food\", false),\n",
    "  Row(8, 1667623942, \"visit\", 74, \"food\", false),\n",
    "  Row(9, 1667623314, \"visit\", 74, \"food\", false),\n",
    "  Row(10, 1667634657, \"visit\", 79, \"tools\", false),\n",
    "  Row(11, 1667643696, \"visit\", 77, \"sport\", false),\n",
    "  Row(12, 1667629904, \"visit\", 77, \"sport\", false),\n",
    ")\n",
    "val df = spark.createDataFrame(spark.sparkContext.parallelize(data), struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afc5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- page_id: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- sign: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2111de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:49:41 INFO CodeGenerator: Code generated in 509.5952 ms\n",
      "22/12/01 18:49:41 INFO SparkContext: Starting job: show at cmd4.sc:1\n",
      "22/12/01 18:49:41 INFO DAGScheduler: Got job 0 (show at cmd4.sc:1) with 1 output partitions\n",
      "22/12/01 18:49:41 INFO DAGScheduler: Final stage: ResultStage 0 (show at cmd4.sc:1)\n",
      "22/12/01 18:49:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:49:41 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:49:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at cmd4.sc:1), which has no missing parents\n",
      "22/12/01 18:49:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 24.5 KiB, free 656.4 MiB)\n",
      "22/12/01 18:49:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 656.4 MiB)\n",
      "22/12/01 18:49:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 13de6c50d362:36135 (size: 9.8 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:49:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:49:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:49:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:49:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6317 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:49:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/12/01 18:49:43 INFO CodeGenerator: Code generated in 174.8367 ms\n",
      "22/12/01 18:49:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1950 bytes result sent to driver\n",
      "22/12/01 18:49:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1075 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:49:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:49:43 INFO DAGScheduler: ResultStage 0 (show at cmd4.sc:1) finished in 1.782 s\n",
      "22/12/01 18:49:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:49:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/12/01 18:49:43 INFO DAGScheduler: Job 0 finished: show at cmd4.sc:1, took 2.018203 s\n",
      "22/12/01 18:49:43 INFO CodeGenerator: Code generated in 38.8552 ms\n",
      "+---+----------+------+-------+------+-----+\n",
      "| id| timestamp|  type|page_id|   tag| sign|\n",
      "+---+----------+------+-------+------+-----+\n",
      "|  1|1667927426| visit|     77| sport| true|\n",
      "|  1|1667327495| click|     77| sport| true|\n",
      "|  1|1667627576|scroll|     77| sport| true|\n",
      "|  1|1667427626|  move|     78|market| true|\n",
      "|  1|1667135426| visit|     77| sport| true|\n",
      "|  1|1667225495| click|     77| sport| true|\n",
      "|  1|1667685576|scroll|     77| sport| true|\n",
      "|  1|1667696626|  move|     78|market| true|\n",
      "|  2|1667718426| visit|     74|  food|false|\n",
      "|  2|1668128495| click|     74|  food|false|\n",
      "|  2|1664928576|scroll|     74|  food|false|\n",
      "|  2|1667378626|  move|     77| sport|false|\n",
      "|  3|1667993426| visit|     78|market| true|\n",
      "|  3|1667229495| click|     78|market| true|\n",
      "|  3|1667329576|scroll|     78|market| true|\n",
      "|  3|1667829626|  move|     77| sport| true|\n",
      "|  4|1667822433| visit|     74|  food|false|\n",
      "|  4|1667722455| click|     74|  food|false|\n",
      "|  4|1667522526|scroll|     74|  food|false|\n",
      "|  4|1667593021| click|     78|market|false|\n",
      "+---+----------+------+-------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c57532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:49:56 INFO CodeGenerator: Code generated in 122.1112 ms\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Registering RDD 5 (show at cmd5.sc:1) as input to shuffle 0\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Got map stage job 1 (show at cmd5.sc:1) with 1 output partitions\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (show at cmd5.sc:1)\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at show at cmd5.sc:1), which has no missing parents\n",
      "22/12/01 18:49:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 38.9 KiB, free 656.3 MiB)\n",
      "22/12/01 18:49:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 656.3 MiB)\n",
      "22/12/01 18:49:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 13de6c50d362:36135 (size: 16.6 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:49:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:49:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at show at cmd5.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:49:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:49:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6301 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:49:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/12/01 18:49:57 INFO CodeGenerator: Code generated in 25.228 ms\n",
      "22/12/01 18:49:57 INFO CodeGenerator: Code generated in 13.9037 ms\n",
      "22/12/01 18:49:57 INFO CodeGenerator: Code generated in 20.9621 ms\n",
      "22/12/01 18:49:57 INFO CodeGenerator: Code generated in 22.3491 ms\n",
      "22/12/01 18:49:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 13de6c50d362:36135 in memory (size: 9.8 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:49:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2496 bytes result sent to driver\n",
      "22/12/01 18:49:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 540 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:49:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:49:57 INFO DAGScheduler: ShuffleMapStage 1 (show at cmd5.sc:1) finished in 0.697 s\n",
      "22/12/01 18:49:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:49:57 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:49:57 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:49:57 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:49:58 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:49:58 INFO CodeGenerator: Code generated in 19.0904 ms\n",
      "22/12/01 18:49:58 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:49:58 INFO CodeGenerator: Code generated in 37.8561 ms\n",
      "22/12/01 18:49:58 INFO SparkContext: Starting job: show at cmd5.sc:1\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Got job 2 (show at cmd5.sc:1) with 1 output partitions\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Final stage: ResultStage 3 (show at cmd5.sc:1)\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at show at cmd5.sc:1), which has no missing parents\n",
      "22/12/01 18:49:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 40.9 KiB, free 656.3 MiB)\n",
      "22/12/01 18:49:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 656.3 MiB)\n",
      "22/12/01 18:49:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 13de6c50d362:36135 (size: 18.0 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:49:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at show at cmd5.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:49:58 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:49:58 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:49:58 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "22/12/01 18:49:58 INFO ShuffleBlockFetcherIterator: Getting 1 (792.0 B) non-empty blocks including 1 (792.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:49:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 35 ms\n",
      "22/12/01 18:49:58 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 6013 bytes result sent to driver\n",
      "22/12/01 18:49:58 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 306 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:49:58 INFO DAGScheduler: ResultStage 3 (show at cmd5.sc:1) finished in 0.425 s\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:49:58 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:49:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "22/12/01 18:49:58 INFO DAGScheduler: Job 2 finished: show at cmd5.sc:1, took 0.505570 s\n",
      "22/12/01 18:49:58 INFO CodeGenerator: Code generated in 49.7467 ms\n",
      "22/12/01 18:49:59 INFO CodeGenerator: Code generated in 37.9155 ms\n",
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    8|\n",
      "|  4|    6|\n",
      "|  3|    4|\n",
      "|  2|    4|\n",
      "|  8|    2|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Вывести топ-5 самых активных посетителей сайта\n",
    "(df.groupBy(\"id\").count().as(\"count\")).orderBy(col(\"count\").desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c7c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:22 INFO CodeGenerator: Code generated in 60.1787 ms\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Registering RDD 11 (show at cmd6.sc:4) as input to shuffle 1\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Got map stage job 3 (show at cmd6.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (show at cmd6.sc:4)\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[11] at show at cmd6.sc:4), which has no missing parents\n",
      "22/12/01 18:50:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 39.5 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 13de6c50d362:36135 (size: 16.9 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[11] at show at cmd6.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6301 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "22/12/01 18:50:22 INFO CodeGenerator: Code generated in 41.6557 ms\n",
      "22/12/01 18:50:22 INFO CodeGenerator: Code generated in 17.214 ms\n",
      "22/12/01 18:50:22 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2410 bytes result sent to driver\n",
      "22/12/01 18:50:22 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 226 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:22 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:22 INFO DAGScheduler: ShuffleMapStage 4 (show at cmd6.sc:4) finished in 0.360 s\n",
      "22/12/01 18:50:22 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:50:22 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:50:22 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:50:22 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:22 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:50:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:50:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:50:23 INFO CodeGenerator: Code generated in 85.2007 ms\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Registering RDD 14 (show at cmd6.sc:4) as input to shuffle 2\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Got map stage job 4 (show at cmd6.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (show at cmd6.sc:4)\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at show at cmd6.sc:4), which has no missing parents\n",
      "22/12/01 18:50:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 59.4 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 13de6c50d362:36135 (size: 22.5 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:23 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at show at cmd6.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:23 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:23 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:23 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
      "22/12/01 18:50:23 INFO ShuffleBlockFetcherIterator: Getting 1 (126.0 B) non-empty blocks including 1 (126.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:50:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "22/12/01 18:50:23 INFO CodeGenerator: Code generated in 43.2268 ms\n",
      "22/12/01 18:50:23 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 4417 bytes result sent to driver\n",
      "22/12/01 18:50:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 189 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:23 INFO DAGScheduler: ShuffleMapStage 6 (show at cmd6.sc:4) finished in 0.280 s\n",
      "22/12/01 18:50:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:50:23 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:50:23 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:50:23 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:50:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/12/01 18:50:23 INFO CodeGenerator: Code generated in 37.0381 ms\n",
      "22/12/01 18:50:23 INFO SparkContext: Starting job: show at cmd6.sc:4\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Got job 5 (show at cmd6.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Final stage: ResultStage 9 (show at cmd6.sc:4)\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[18] at show at cmd6.sc:4), which has no missing parents\n",
      "22/12/01 18:50:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 47.8 KiB, free 656.1 MiB)\n",
      "22/12/01 18:50:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 656.1 MiB)\n",
      "22/12/01 18:50:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 13de6c50d362:36135 (size: 19.6 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[18] at show at cmd6.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:23 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:23 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:23 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)\n",
      "22/12/01 18:50:24 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:50:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "22/12/01 18:50:24 INFO CodeGenerator: Code generated in 25.4845 ms\n",
      "22/12/01 18:50:24 INFO CodeGenerator: Code generated in 20.6912 ms\n",
      "22/12/01 18:50:24 INFO CodeGenerator: Code generated in 28.444 ms\n",
      "22/12/01 18:50:24 INFO CodeGenerator: Code generated in 19.0185 ms\n",
      "22/12/01 18:50:24 INFO CodeGenerator: Code generated in 21.643 ms\n",
      "22/12/01 18:50:24 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 5011 bytes result sent to driver\n",
      "22/12/01 18:50:24 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 450 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:24 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:24 INFO DAGScheduler: ResultStage 9 (show at cmd6.sc:4) finished in 0.562 s\n",
      "22/12/01 18:50:24 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:50:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "22/12/01 18:50:24 INFO DAGScheduler: Job 5 finished: show at cmd6.sc:4, took 0.588608 s\n",
      "22/12/01 18:50:24 INFO CodeGenerator: Code generated in 21.5299 ms\n",
      "+-----+---+-----------------+\n",
      "| sign|sum|       percentage|\n",
      "+-----+---+-----------------+\n",
      "| true| 12|38.70967741935484|\n",
      "|false| 19|61.29032258064516|\n",
      "+-----+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Посчитать процент посетителей, у которых есть ЛК\n",
    "df.groupBy(\"sign\").count()\n",
    "  .groupBy(\"sign\")\n",
    "  .agg(sum(\"count\").alias(\"sum\"))\n",
    "  .withColumn(\"percentage\", col(\"sum\") * 100 / sum(\"sum\").over()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e4ee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:50:32 INFO CodeGenerator: Code generated in 60.0094 ms\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Registering RDD 20 (show at cmd7.sc:4) as input to shuffle 3\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Got map stage job 6 (show at cmd7.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (show at cmd7.sc:4)\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[20] at show at cmd7.sc:4), which has no missing parents\n",
      "22/12/01 18:50:32 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.9 KiB, free 656.0 MiB)\n",
      "22/12/01 18:50:32 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 656.0 MiB)\n",
      "22/12/01 18:50:32 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 13de6c50d362:36135 (size: 17.4 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:32 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[20] at show at cmd7.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:32 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:32 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 6) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6301 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:32 INFO Executor: Running task 0.0 in stage 10.0 (TID 6)\n",
      "22/12/01 18:50:32 INFO CodeGenerator: Code generated in 38.6471 ms\n",
      "22/12/01 18:50:32 INFO Executor: Finished task 0.0 in stage 10.0 (TID 6). 2466 bytes result sent to driver\n",
      "22/12/01 18:50:32 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 6) in 195 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:32 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:32 INFO DAGScheduler: ShuffleMapStage 10 (show at cmd7.sc:4) finished in 0.307 s\n",
      "22/12/01 18:50:32 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:50:32 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:50:32 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:50:32 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:50:32 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:50:32 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:50:32 INFO CodeGenerator: Code generated in 40.9247 ms\n",
      "22/12/01 18:50:32 INFO SparkContext: Starting job: show at cmd7.sc:4\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Got job 7 (show at cmd7.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Final stage: ResultStage 12 (show at cmd7.sc:4)\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[24] at show at cmd7.sc:4), which has no missing parents\n",
      "22/12/01 18:50:32 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 42.6 KiB, free 656.0 MiB)\n",
      "22/12/01 18:50:32 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 656.0 MiB)\n",
      "22/12/01 18:50:32 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 13de6c50d362:36135 (size: 18.6 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:32 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[24] at show at cmd7.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:32 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:32 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 7) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:32 INFO Executor: Running task 0.0 in stage 12.0 (TID 7)\n",
      "22/12/01 18:50:32 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:50:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "22/12/01 18:50:33 INFO Executor: Finished task 0.0 in stage 12.0 (TID 7). 5987 bytes result sent to driver\n",
      "22/12/01 18:50:33 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 7) in 134 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:33 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:33 INFO DAGScheduler: ResultStage 12 (show at cmd7.sc:4) finished in 0.202 s\n",
      "22/12/01 18:50:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:50:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "22/12/01 18:50:33 INFO DAGScheduler: Job 7 finished: show at cmd7.sc:4, took 0.231798 s\n",
      "22/12/01 18:50:33 INFO CodeGenerator: Code generated in 37.9782 ms\n",
      "22/12/01 18:50:33 INFO CodeGenerator: Code generated in 30.1034 ms\n",
      "+-------+-----+\n",
      "|page_id|count|\n",
      "+-------+-----+\n",
      "|     78|    2|\n",
      "|     74|    2|\n",
      "|     77|    2|\n",
      "|     79|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Вывести топ-5 страниц сайта по показателю среднего кол-ва кликов на данной странице\n",
    "df.filter(df(\"type\")===\"click\")\n",
    "  .groupBy(\"page_id\")\n",
    "  .count()\n",
    "  .orderBy( col(\"count\").desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80069d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 13de6c50d362:36135 in memory (size: 19.6 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 13de6c50d362:36135 in memory (size: 17.4 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 13de6c50d362:36135 in memory (size: 22.5 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 13de6c50d362:36135 in memory (size: 18.0 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 13de6c50d362:36135 in memory (size: 18.6 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 13de6c50d362:36135 in memory (size: 16.9 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:50:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 13de6c50d362:36135 in memory (size: 16.6 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:50:40 INFO CodeGenerator: Code generated in 34.8411 ms\n",
      "22/12/01 18:50:40 INFO SparkContext: Starting job: show at cmd8.sc:3\n",
      "22/12/01 18:50:40 INFO DAGScheduler: Got job 8 (show at cmd8.sc:3) with 1 output partitions\n",
      "22/12/01 18:50:40 INFO DAGScheduler: Final stage: ResultStage 13 (show at cmd8.sc:3)\n",
      "22/12/01 18:50:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:50:40 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:40 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[26] at show at cmd8.sc:3), which has no missing parents\n",
      "22/12/01 18:50:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 27.3 KiB, free 656.4 MiB)\n",
      "22/12/01 18:50:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 656.4 MiB)\n",
      "22/12/01 18:50:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 13de6c50d362:36135 (size: 11.0 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:50:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[26] at show at cmd8.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:41 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:41 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 8) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6312 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:41 INFO Executor: Running task 0.0 in stage 13.0 (TID 8)\n",
      "22/12/01 18:50:41 INFO Executor: Finished task 0.0 in stage 13.0 (TID 8). 2294 bytes result sent to driver\n",
      "22/12/01 18:50:41 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 8) in 89 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:41 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:41 INFO DAGScheduler: ResultStage 13 (show at cmd8.sc:3) finished in 0.163 s\n",
      "22/12/01 18:50:41 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:50:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "22/12/01 18:50:41 INFO DAGScheduler: Job 8 finished: show at cmd8.sc:3, took 0.180695 s\n",
      "22/12/01 18:50:41 INFO CodeGenerator: Code generated in 54.7026 ms\n",
      "+---+----------+------+-------+------+-----+-------------------+-------+\n",
      "| id| timestamp|  type|page_id|   tag| sign|          NormalDay|PartDay|\n",
      "+---+----------+------+-------+------+-----+-------------------+-------+\n",
      "|  1|1667927426| visit|     77| sport| true|2022-11-08 17:10:26|      4|\n",
      "|  1|1667327495| click|     77| sport| true|2022-11-01 18:31:35|      4|\n",
      "|  1|1667627576|scroll|     77| sport| true|2022-11-05 05:52:56|      1|\n",
      "|  1|1667427626|  move|     78|market| true|2022-11-02 22:20:26|      5|\n",
      "|  1|1667135426| visit|     77| sport| true|2022-10-30 13:10:26|      3|\n",
      "|  1|1667225495| click|     77| sport| true|2022-10-31 14:11:35|      3|\n",
      "|  1|1667685576|scroll|     77| sport| true|2022-11-05 21:59:36|      5|\n",
      "|  1|1667696626|  move|     78|market| true|2022-11-06 01:03:46|      0|\n",
      "|  2|1667718426| visit|     74|  food|false|2022-11-06 07:07:06|      1|\n",
      "|  2|1668128495| click|     74|  food|false|2022-11-11 01:01:35|      0|\n",
      "|  2|1664928576|scroll|     74|  food|false|2022-10-05 00:09:36|      0|\n",
      "|  2|1667378626|  move|     77| sport|false|2022-11-02 08:43:46|      2|\n",
      "|  3|1667993426| visit|     78|market| true|2022-11-09 11:30:26|      2|\n",
      "|  3|1667229495| click|     78|market| true|2022-10-31 15:18:15|      3|\n",
      "|  3|1667329576|scroll|     78|market| true|2022-11-01 19:06:16|      4|\n",
      "|  3|1667829626|  move|     77| sport| true|2022-11-07 14:00:26|      3|\n",
      "|  4|1667822433| visit|     74|  food|false|2022-11-07 12:00:33|      3|\n",
      "|  4|1667722455| click|     74|  food|false|2022-11-06 08:14:15|      2|\n",
      "|  4|1667522526|scroll|     74|  food|false|2022-11-04 00:42:06|      0|\n",
      "|  4|1667593021| click|     78|market|false|2022-11-04 20:17:01|      5|\n",
      "+---+----------+------+-------+------+-----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, timestamp: int ... 6 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Добавьте столбец к фрейму данных со значением временного диапазона в рамках суток с размером окна – 4 часа(0-4, 4-8, 8-12 и т.д.)\n",
    "val df2=df.withColumn(\"NormalDay\", from_unixtime(col(\"timestamp\"),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "  .withColumn(\"PartDay\", (hour(from_unixtime(col(\"timestamp\"),\"yyyy-MM-dd HH:mm:ss\"))/4).cast(\"int\"))\n",
    "println(df2.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e359bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:50:47 INFO CodeGenerator: Code generated in 59.1844 ms\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Registering RDD 28 (collect at cmd9.sc:4) as input to shuffle 4\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Got map stage job 9 (collect at cmd9.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (collect at cmd9.sc:4)\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[28] at collect at cmd9.sc:4), which has no missing parents\n",
      "22/12/01 18:50:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 43.9 KiB, free 656.3 MiB)\n",
      "22/12/01 18:50:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 656.3 MiB)\n",
      "22/12/01 18:50:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 13de6c50d362:36135 (size: 18.9 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:50:47 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[28] at collect at cmd9.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:47 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:47 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6301 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:47 INFO Executor: Running task 0.0 in stage 14.0 (TID 9)\n",
      "22/12/01 18:50:48 INFO Executor: Finished task 0.0 in stage 14.0 (TID 9). 2410 bytes result sent to driver\n",
      "22/12/01 18:50:48 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 185 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:48 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:48 INFO DAGScheduler: ShuffleMapStage 14 (collect at cmd9.sc:4) finished in 0.291 s\n",
      "22/12/01 18:50:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:50:48 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:50:48 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:50:48 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:50:48 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:50:48 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:50:48 INFO CodeGenerator: Code generated in 34.5626 ms\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Registering RDD 31 (collect at cmd9.sc:4) as input to shuffle 5\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Got map stage job 10 (collect at cmd9.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (collect at cmd9.sc:4)\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[31] at collect at cmd9.sc:4), which has no missing parents\n",
      "22/12/01 18:50:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 46.6 KiB, free 656.3 MiB)\n",
      "22/12/01 18:50:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 13de6c50d362:36135 (size: 20.2 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:50:48 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[31] at collect at cmd9.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:48 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:48 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 10) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:48 INFO Executor: Running task 0.0 in stage 16.0 (TID 10)\n",
      "22/12/01 18:50:48 INFO ShuffleBlockFetcherIterator: Getting 1 (384.0 B) non-empty blocks including 1 (384.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "22/12/01 18:50:48 INFO Executor: Finished task 0.0 in stage 16.0 (TID 10). 4081 bytes result sent to driver\n",
      "22/12/01 18:50:48 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 10) in 113 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:48 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:48 INFO DAGScheduler: ShuffleMapStage 16 (collect at cmd9.sc:4) finished in 0.208 s\n",
      "22/12/01 18:50:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:50:48 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:50:48 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:50:48 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:50:48 INFO CodeGenerator: Code generated in 22.7588 ms\n",
      "22/12/01 18:50:48 INFO SparkContext: Starting job: collect at cmd9.sc:4\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Got job 11 (collect at cmd9.sc:4) with 1 output partitions\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Final stage: ResultStage 19 (collect at cmd9.sc:4)\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[34] at collect at cmd9.sc:4), which has no missing parents\n",
      "22/12/01 18:50:48 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.7 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:48 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:48 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 13de6c50d362:36135 (size: 5.7 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:48 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[34] at collect at cmd9.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:48 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:48 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 11) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:48 INFO Executor: Running task 0.0 in stage 19.0 (TID 11)\n",
      "22/12/01 18:50:48 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:50:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "22/12/01 18:50:48 INFO Executor: Finished task 0.0 in stage 19.0 (TID 11). 2656 bytes result sent to driver\n",
      "22/12/01 18:50:48 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 11) in 67 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:48 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:48 INFO DAGScheduler: ResultStage 19 (collect at cmd9.sc:4) finished in 0.091 s\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:50:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "22/12/01 18:50:48 INFO DAGScheduler: Job 11 finished: collect at cmd9.sc:4, took 0.130183 s\n",
      "22/12/01 18:50:48 INFO CodeGenerator: Code generated in 57.7349 ms\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Registering RDD 36 (show at cmd9.sc:5) as input to shuffle 6\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Got map stage job 12 (show at cmd9.sc:5) with 1 output partitions\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (show at cmd9.sc:5)\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[36] at show at cmd9.sc:5), which has no missing parents\n",
      "22/12/01 18:50:49 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 44.4 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:49 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 656.2 MiB)\n",
      "22/12/01 18:50:49 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 13de6c50d362:36135 (size: 19.1 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:49 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[36] at show at cmd9.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:49 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:49 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 12) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6301 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:49 INFO Executor: Running task 0.0 in stage 20.0 (TID 12)\n",
      "22/12/01 18:50:49 INFO Executor: Finished task 0.0 in stage 20.0 (TID 12). 2410 bytes result sent to driver\n",
      "22/12/01 18:50:49 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 12) in 116 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:49 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:49 INFO DAGScheduler: ShuffleMapStage 20 (show at cmd9.sc:5) finished in 0.174 s\n",
      "22/12/01 18:50:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:50:49 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:50:49 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:50:49 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:50:49 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:50:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:50:49 INFO CodeGenerator: Code generated in 51.7573 ms\n",
      "22/12/01 18:50:49 INFO SparkContext: Starting job: show at cmd9.sc:5\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Got job 13 (show at cmd9.sc:5) with 1 output partitions\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Final stage: ResultStage 22 (show at cmd9.sc:5)\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[39] at show at cmd9.sc:5), which has no missing parents\n",
      "22/12/01 18:50:49 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 44.8 KiB, free 656.1 MiB)\n",
      "22/12/01 18:50:49 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 656.1 MiB)\n",
      "22/12/01 18:50:49 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 13de6c50d362:36135 (size: 19.5 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:50:49 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[39] at show at cmd9.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:50:49 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:50:49 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 13) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:50:49 INFO Executor: Running task 0.0 in stage 22.0 (TID 13)\n",
      "22/12/01 18:50:49 INFO ShuffleBlockFetcherIterator: Getting 1 (384.0 B) non-empty blocks including 1 (384.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:50:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "22/12/01 18:50:49 INFO Executor: Finished task 0.0 in stage 22.0 (TID 13). 3642 bytes result sent to driver\n",
      "22/12/01 18:50:49 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 13) in 86 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:50:49 INFO DAGScheduler: ResultStage 22 (show at cmd9.sc:5) finished in 0.163 s\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:50:49 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:50:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "22/12/01 18:50:49 INFO DAGScheduler: Job 13 finished: show at cmd9.sc:5, took 0.203600 s\n",
      "+-------+-----+\n",
      "|PartDay|count|\n",
      "+-------+-----+\n",
      "|      1|    6|\n",
      "|      3|    6|\n",
      "|      0|    6|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Выведите временной промежуток на основе предыдущего задания, в течение которого было больше всего активностей на сайте.\n",
    "(df2.groupBy(\"PartDay\").count().as(\"count\"))\n",
    "  .where(col(\"count\") === (df2.groupBy(\"PartDay\").count().as(\"count\"))\n",
    "                            .agg(max(\"count\").alias(\"maxOf\"))\n",
    "                            .select(\"maxOf\").collect()\n",
    "                            .head.getLong(0))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57822f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mstruct2\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"id\"\u001b[39m, IntegerType, false, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"User_id\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FIO\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"dateofcreate\"\u001b[39m, IntegerType, true, {})\n",
       ")\n",
       "\u001b[36mdata2\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  [1,1,Иванов Иван Иванович,1665023314],\n",
       "  [2,2,Петров Петр Петрович,1664123314],\n",
       "  [3,3,Сергеев Сергей Сергеевич,1665923314],\n",
       "  [4,4,Сидоров Сидор Сидорович,1661723314],\n",
       "  [5,5,Дмитриев Дмитрий Дмитриевич,1666823314]\n",
       ")\n",
       "\u001b[36mdfUs\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, User_id: int ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Создайте второй фрейм данных, который будет содержать информацию о ЛК посетителя сайта со следующим списком атрибутов\n",
    "val struct2 = (new StructType)\n",
    "  .add(StructField(\"id\", IntegerType, false))\n",
    "  .add(StructField(\"User_id\", IntegerType, true))\n",
    "  .add(StructField(\"FIO\", StringType, true))\n",
    "  .add(StructField(\"dateofcreate\", IntegerType, true))\n",
    "\n",
    "val data2 = Seq(\n",
    "  Row(1, 1, \"Иванов Иван Иванович\", 1665023314),\n",
    "  Row(2, 2, \"Петров Петр Петрович\", 1664123314),\n",
    "  Row(3, 3, \"Сергеев Сергей Сергеевич\", 1665923314),\n",
    "  Row(4, 4, \"Сидоров Сидор Сидорович\", 1661723314),\n",
    "  Row(5, 5, \"Дмитриев Дмитрий Дмитриевич\", 1666823314),\n",
    ")\n",
    "val dfUs = spark.createDataFrame(spark.sparkContext.parallelize(data2), struct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebdccf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 18:51:44 INFO CodeGenerator: Code generated in 7.362 ms\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Registering RDD 45 (show at cmd12.sc:3) as input to shuffle 7\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Got map stage job 14 (show at cmd12.sc:3) with 1 output partitions\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (show at cmd12.sc:3)\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[45] at show at cmd12.sc:3), which has no missing parents\n",
      "22/12/01 18:51:44 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 25.5 KiB, free 656.1 MiB)\n",
      "22/12/01 18:51:44 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.6 KiB, free 656.1 MiB)\n",
      "22/12/01 18:51:44 INFO CodeGenerator: Code generated in 36.0994 ms\n",
      "22/12/01 18:51:44 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 13de6c50d362:36135 (size: 10.6 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:44 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[45] at show at cmd12.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:51:44 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:51:44 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 14) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 6306 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:51:44 INFO Executor: Running task 0.0 in stage 23.0 (TID 14)\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Registering RDD 47 (show at cmd12.sc:3) as input to shuffle 8\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Got map stage job 15 (show at cmd12.sc:3) with 1 output partitions\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (show at cmd12.sc:3)\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:51:44 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[47] at show at cmd12.sc:3), which has no missing parents\n",
      "22/12/01 18:51:44 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 23.8 KiB, free 656.0 MiB)\n",
      "22/12/01 18:51:44 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 656.0 MiB)\n",
      "22/12/01 18:51:44 INFO Executor: Finished task 0.0 in stage 23.0 (TID 14). 1955 bytes result sent to driver\n",
      "22/12/01 18:51:44 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 14) in 181 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:51:44 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 13de6c50d362:36135 (size: 10.5 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:44 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:51:45 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[47] at show at cmd12.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:51:45 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:51:45 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 15) (13de6c50d362, executor driver, partition 0, PROCESS_LOCAL, 5019 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:51:45 INFO Executor: Running task 0.0 in stage 24.0 (TID 15)\n",
      "22/12/01 18:51:45 INFO DAGScheduler: ShuffleMapStage 23 (show at cmd12.sc:3) finished in 0.322 s\n",
      "22/12/01 18:51:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:51:45 INFO DAGScheduler: running: Set(ShuffleMapStage 24)\n",
      "22/12/01 18:51:45 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:51:45 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 13de6c50d362:36135 in memory (size: 5.7 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 13de6c50d362:36135 in memory (size: 20.2 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 13de6c50d362:36135 in memory (size: 19.1 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:45 INFO CodeGenerator: Code generated in 223.7792 ms\n",
      "22/12/01 18:51:45 INFO Executor: Finished task 0.0 in stage 24.0 (TID 15). 2041 bytes result sent to driver\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 13de6c50d362:36135 in memory (size: 18.9 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:45 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 15) in 445 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:51:45 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:51:45 INFO DAGScheduler: ShuffleMapStage 24 (show at cmd12.sc:3) finished in 0.675 s\n",
      "22/12/01 18:51:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:51:45 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:51:45 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:51:45 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 13de6c50d362:36135 in memory (size: 10.6 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:51:45 INFO ShufflePartitionsUtil: For shuffle(7, 8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 13de6c50d362:36135 in memory (size: 19.5 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:51:45 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 13de6c50d362:36135 in memory (size: 11.0 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:51:45 INFO CodeGenerator: Code generated in 50.4569 ms\n",
      "22/12/01 18:51:45 INFO CodeGenerator: Code generated in 24.2266 ms\n",
      "22/12/01 18:51:45 INFO CodeGenerator: Code generated in 19.3338 ms\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Registering RDD 54 (show at cmd12.sc:3) as input to shuffle 9\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Got map stage job 16 (show at cmd12.sc:3) with 1 output partitions\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (show at cmd12.sc:3)\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25, ShuffleMapStage 26)\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:51:45 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[54] at show at cmd12.sc:3), which has no missing parents\n",
      "22/12/01 18:51:45 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 54.8 KiB, free 656.3 MiB)\n",
      "22/12/01 18:51:45 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 656.3 MiB)\n",
      "22/12/01 18:51:45 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 13de6c50d362:36135 (size: 22.5 KiB, free: 656.4 MiB)\n",
      "22/12/01 18:51:46 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[54] at show at cmd12.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:51:46 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:51:46 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 16) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4724 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:51:46 INFO Executor: Running task 0.0 in stage 27.0 (TID 16)\n",
      "22/12/01 18:51:46 INFO ShuffleBlockFetcherIterator: Getting 1 (306.0 B) non-empty blocks including 1 (306.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 37.3437 ms\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 19.2871 ms\n",
      "22/12/01 18:51:46 INFO ShuffleBlockFetcherIterator: Getting 1 (494.0 B) non-empty blocks including 1 (494.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 32.104 ms\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 13.4506 ms\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 29.9594 ms\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 23.2217 ms\n",
      "22/12/01 18:51:46 INFO Executor: Finished task 0.0 in stage 27.0 (TID 16). 5080 bytes result sent to driver\n",
      "22/12/01 18:51:46 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 16) in 434 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:51:46 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:51:46 INFO DAGScheduler: ShuffleMapStage 27 (show at cmd12.sc:3) finished in 0.517 s\n",
      "22/12/01 18:51:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/12/01 18:51:46 INFO DAGScheduler: running: Set()\n",
      "22/12/01 18:51:46 INFO DAGScheduler: waiting: Set()\n",
      "22/12/01 18:51:46 INFO DAGScheduler: failed: Set()\n",
      "22/12/01 18:51:46 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/12/01 18:51:46 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 47.9812 ms\n",
      "22/12/01 18:51:46 INFO SparkContext: Starting job: show at cmd12.sc:3\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Got job 17 (show at cmd12.sc:3) with 1 output partitions\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Final stage: ResultStage 31 (show at cmd12.sc:3)\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[57] at show at cmd12.sc:3), which has no missing parents\n",
      "22/12/01 18:51:46 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 50.0 KiB, free 656.2 MiB)\n",
      "22/12/01 18:51:46 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 656.2 MiB)\n",
      "22/12/01 18:51:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 13de6c50d362:36135 (size: 20.3 KiB, free: 656.3 MiB)\n",
      "22/12/01 18:51:46 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[57] at show at cmd12.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "22/12/01 18:51:46 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "22/12/01 18:51:46 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 17) (13de6c50d362, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/12/01 18:51:46 INFO Executor: Running task 0.0 in stage 31.0 (TID 17)\n",
      "22/12/01 18:51:46 INFO ShuffleBlockFetcherIterator: Getting 1 (291.0 B) non-empty blocks including 1 (291.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/12/01 18:51:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "22/12/01 18:51:46 INFO Executor: Finished task 0.0 in stage 31.0 (TID 17). 5846 bytes result sent to driver\n",
      "22/12/01 18:51:46 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 17) in 67 ms on 13de6c50d362 (executor driver) (1/1)\n",
      "22/12/01 18:51:46 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "22/12/01 18:51:46 INFO DAGScheduler: ResultStage 31 (show at cmd12.sc:3) finished in 0.143 s\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/12/01 18:51:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "22/12/01 18:51:46 INFO DAGScheduler: Job 17 finished: show at cmd12.sc:3, took 0.174333 s\n",
      "22/12/01 18:51:46 INFO CodeGenerator: Code generated in 25.287 ms\n",
      "+------------------------+\n",
      "|FIO                     |\n",
      "+------------------------+\n",
      "|Сергеев Сергей Сергеевич|\n",
      "|Петров Петр Петрович    |\n",
      "|Иванов Иван Иванович    |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Вывести фамилии посетителей, которые читали хотя бы одну новость про спорт.\n",
    "df.join(dfUs, df(\"id\") === dfUs(\"User_id\"), \"inner\")\n",
    "  .filter(df(\"tag\") === \"sport\")\n",
    "  .select(\"FIO\").distinct.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ad63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- User_id: integer (nullable = true)\n",
      " |-- FIO: string (nullable = true)\n",
      " |-- dateofcreate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
